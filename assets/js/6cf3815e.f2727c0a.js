"use strict";(self.webpackChunklinkis_web_apache=self.webpackChunklinkis_web_apache||[]).push([[69424],{3905:function(e,t,r){r.d(t,{Zo:function(){return c},kt:function(){return m}});var a=r(67294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var p=a.createContext({}),l=function(e){var t=a.useContext(p),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},c=function(e){var t=l(e.components);return a.createElement(p.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,o=e.originalType,p=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=l(r),m=n,f=d["".concat(p,".").concat(m)]||d[m]||u[m]||o;return r?a.createElement(f,i(i({ref:t},c),{},{components:r})):a.createElement(f,i({ref:t},c))}));function m(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=r.length,i=new Array(o);i[0]=d;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s.mdxType="string"==typeof e?e:n,i[1]=s;for(var l=2;l<o;l++)i[l]=r[l];return a.createElement.apply(null,i)}return a.createElement.apply(null,r)}d.displayName="MDXCreateElement"},79356:function(e,t,r){r.r(t),r.d(t,{frontMatter:function(){return s},contentTitle:function(){return p},metadata:function(){return l},toc:function(){return c},default:function(){return d}});var a=r(87462),n=r(63366),o=(r(67294),r(3905)),i=["components"],s={title:"Spark Engine File Import Export",sidebar_position:2},p=void 0,l={unversionedId:"architecture/ujes/file-import-and-export-structure",id:"version-0.11.0/architecture/ujes/file-import-and-export-structure",isDocsHomePage:!1,title:"Spark Engine File Import Export",description:"1 Background",source:"@site/versioned_docs/version-0.11.0/architecture/ujes/file-import-and-export-structure.md",sourceDirName:"architecture/ujes",slug:"/architecture/ujes/file-import-and-export-structure",permalink:"/docs/0.11.0/architecture/ujes/file-import-and-export-structure",editUrl:"https://github.com/apache/incubator-linkis-website/edit/dev/versioned_docs/version-0.11.0/architecture/ujes/file-import-and-export-structure.md",tags:[],version:"0.11.0",sidebarPosition:2,frontMatter:{title:"Spark Engine File Import Export",sidebar_position:2},sidebar:"version-1.0.2/tutorialSidebar",previous:{title:"UJES Design",permalink:"/docs/0.11.0/architecture/ujes/ujes-design"},next:{title:"Asynchronous Pool Call",permalink:"/docs/0.11.0/architecture/ujes/asynchronous-thread-pool"}},c=[{value:"1 Background",id:"1-background",children:[]},{value:"2 Thinking",id:"2-thinking",children:[]},{value:"3 Implementation",id:"3-implementation",children:[{value:"3.1 Export",id:"31-export",children:[]},{value:"3.2 Import",id:"32-import",children:[]}]}],u={toc:c};function d(e){var t=e.components,s=(0,n.Z)(e,i);return(0,o.kt)("wrapper",(0,a.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"1-background"},"1 Background"),(0,o.kt)("p",null,"Data analysts or data warehouses are often required to export data from databases to Excel files for data analysis, or to export data to Excel for users or co-operators."),(0,o.kt)("p",null,"Furthermore, users often need to undertake joint analyses of data files such as CSV, Excel and online Hive databases, which need to be imported into the Hive database."),(0,o.kt)("p",null,"For more confidential industries, such as banks, data exports often require sensitive export fields such as identity cards, mobile phone numbers."),(0,o.kt)("h2",{id:"2-thinking"},"2 Thinking"),(0,o.kt)("p",null,"Using Spark's distributed computing capability and supporting DataSource, which connects multiple data sources."),(0,o.kt)("h2",{id:"3-implementation"},"3 Implementation"),(0,o.kt)("h3",{id:"31-export"},"3.1 Export"),(0,o.kt)("p",null,"The export process is shown below in graph\uff1a"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Export process",src:r(18309).Z})),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"The user selects the corresponding data source and the corresponding data form to be exported, such as the user order form in the\uff1aMysql library;")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"User defines the query statement of data to be exported from the data table, as well as the data transformation to the specified column."),(0,o.kt)("p",{parentName:"li"},"For example,\uff1adefines the export of order forms for the last six months and dissociates user information;")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"User selects file formats and output paths to export, e.g.\uff1aexport user order form to excel, path to /home/username/orders.xlsx")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Spark read corresponding data based on user configured data sources and tables and querying statements. DataSource supports multiple data storage components such as\uff1aHive,Mysql, Oracle,HDF,Hbase,Mongodb")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"The data is then processed to DataFrame according to the data conversion format configured by the user")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Gets the file write object according to the file format type of the user configuration, e.g.\uff1asupports the file writing object for Spark's Excel.Writer's support for multiple file formats such as Excel, exce, Json")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Write the corresponding data via writer to the corresponding destination, e.g.\uff1a/home/username/orders.xlsx."))),(0,o.kt)("h3",{id:"32-import"},"3.2 Import"),(0,o.kt)("p",null,"Import process below\uff1a"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Import process",src:r(59945).Z})),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"The user selects the exported file. File readers will read from incoming files: e.g.\uff1a/home/username/orders.xlsx;")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Readers read the contents of the previous N line for data type extrapolations, such as reading 10 lines.Reader supports reading in multiple file formats.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Data type extrapolators use the first 10 lines of incoming data type to determine the type of data in each column. The method is to determine the data type in each row by determining the type of data and ultimately by determining the number of times the type appears, and to return to the user."),(0,o.kt)("p",{parentName:"li"},"e.g.\uff1auser\uff1aString,orderId\uff1aInt;")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"User selected data sources to import, e.g.\uff1aMysql.Import data also supports multiple selections;")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"The user chooses whether to create a new tree or rewrite the data or add the data.Select user order form and select data appending;")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"User-defined data import transformation format and imported column information, such as\uff1adecrypting user information")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"The scheme uses Spark and transforms the file to DataFrame via user incoming data to events and column information;")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Generate the corresponding Datasupply via the data source selected by the user")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Import processed DataFrame via Datasource to the corresponding data source, e.g.\uff1aMysql library."))))}d.isMDXComponent=!0},18309:function(e,t,r){t.Z=r.p+"assets/images/export_process-ead84125fd4b357c47bb7aa5444c34ae.png"},59945:function(e,t,r){t.Z=r.p+"assets/images/import_process-3f4b657711b2dd8a2b757283f983c634.png"}}]);